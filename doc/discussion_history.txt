
2018/01/24

* 민혜생각의 현재의 최적 parameter
one_dim = 7
mu = 4 / 5
init_txr = 2
action_space = [-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1]
환경에서는
    epsilon = 0.4 # explore ratio
    utility_coeff = 2 #0.95  # weight on goodput
    utility_pos_coeff = 1  # to make reward to be positive

* goodput과 energy 사이의 trade-off를 보여줄 만한 내용있는지? --> utility_coeff를 사용하는 방법?

======================
2018/1/21 off-line meeting

Future Schedule 
* 기본 matlab code 완성. 
* 그 후에, 여러 아이디어 discussion을 해서 적용을 해본 후 그 중에 성능 좋은 것을 뽑아냄. 


용어정리
* step: 하나의 action을 취하는 set
* episod: step들의 집합. 우리의 경우는 number of node generation (= network member changes)가 poisson distribution으로 새롭게 나오는 것. 

논문에 추가할 새로운 아이디어들
* explore vs exploit : epsilon값을 constant로 주는 or decay되는 셋팅 --> 이론적인 startegy를 적용 ex) UCB, tomson's sampling 
--> 처음에는 가져다 쓰는것 만으로 ok. best will be 우리 상황에 optimal한 방법을 proof 해주는 것
* multi agent reinforcement learning 
* member change 상황에서 new member나 사라진 멤버들을 어떻게 처리할지에 대한 방법 제안. 
* 아주 나중에 idea: state 의 정의를 adjacent matrix를 가져다 사용하는 건? 
--> 왜 이런생각이 났냐면 deep learning을 쓰려면 high dimension의 input (state definition)이 필요해서!! 

==========================

2018/1/8 conference call
* discussion 내용: 구현 중 Q&A

* 현재 진행 상황:
- Q table 기반 Q-learning 모듈 구현
- Environment 구현, action이 주어지면 그에 따라 reward, next state return하도록
- main 부분 구현하여 전체 구조 잡아놓음 

* state 구현 관련 의문 사항(주현) 및 답변, 피드백(민혜)
- adj_matrix의 diagonal term을 1로 넣어주기
- beta (link failure rate)는 우선 0으로 고정시켜놓고 구현
- player 마다 agent를 의미,
- 각 player 마다 연결된 node 수가 state (state[p] = sum(adj_matrix[p,:])
- action도 각 player마다 tx_r가 있어야함, 처음 initializing 할때만 같게 주어짐.

=================================

2017/11/27 conference call
* discussion 내용: 다른 deep RL 논문의 경우는 state와 action의 수가 매우 많음

* 현재상황: NW environment 부분 python 변환 완료
* next step: RL 부분 코딩 필요 - Q learning algoirhtm - Q table 기반으로 simple example 구현 
--> simple example로 부터 시작해서 점점 더 복잡화 시키는 방향으로 발전

* time line
- 민혜 학회: 12/3 - 9
- 주현 휴가: 12/9 - 16
1. 주현 휴가 전까지 RL 부분 코딩
2. 민혜 주현 휴가동안 코딩 catch up 및 아이디어/발전내용 찾기


==============================
2017/11/11 meeting

* Contents
- Evolutionary Network Formation for Dynamic Wireless Networks with Network Coding 논문 정리 및 Q&A
- paper idea 정리
- discussion 내용
	- state: the number of physically located node
	- action: difference of transmission range
	- reward(R): throughput (can be delayed feedback, should be implemented in simultation code)
	- utility: utility = u + w * R - (1 - w) *a

	- no transition probability (model-free, reinforment learning)
	- experiment setup: change beta(link failure rate), change node location
	- estimate Q(s,a) value based on throughput reward
	- if link failure rate is changed, adapt transmission range by using Q learning based on throughput reward


* To do
민혜:
- 기존 코드 정리 및 업로드 

주현:
- ICML application paper 분석 및 공유
- github 정리
