https://docs.google.com/document/d/1ZmyRliXf7kwcV5vJDj3YmxJgeLdcQ9qzkEVxDS2ybcw/edit


========================
2018-02-04

민혜 comments

이상한점
1. 아래 수정내역 1에서 수정한 reward function 설정하에,
(utility_pos_coeff = 0 & )utility_coeff =1 일때엔 무조건 action을 positive 크게 가져가는게 좋을텐데 (energy 소모는 신경안쓰는 경우이므로), 그럼에도 불구하고 energy 줄이는 방향으로 가는 node 존재?
--> 이 energy 줄이는 방향으로 가는걸 없애주려면 utility_pos_coeff를 5이상으로 잡아줘야함!
--> reward가 음수로 나와도 상관 없을텐데 utility_pos_coeff 값에 영향을 받는듯?
--> 현재 설정에서는 utility_pos_coeff 의 영향이 크지 않아 보임
* 현재설정:
utility_pos_coeff = 5
reward = self.utility_pos_coeff + self.utility_coeff * 20 * (goodput - self.last_goodput) - (1-self.utility_coeff) * action
ep_length = 200
epsilon = {'epsilon_start': 1.0, 'epsilon_end': 0.01, 'epsilon_step': 100}


부탁  - To do list
위의 *현재설정*에서 아래 환경을 시행해 보는데,
epsode 101~200 에서의 energy, goodput, connectivity ratio 평균을
num_ep = 100 으로 확인해보기 (100설정 이유 - 우선 빨리 결과 봐보려고!)
#utility coeff
python main.py --nw_size 8 --coeff 1.0 --beta 0.0
python main.py --nw_size 8 --coeff 0.8 --beta 0.0
python main.py --nw_size 8 --coeff 0.6 --beta 0.0
python main.py --nw_size 8 --coeff 0.4 --beta 0.0
python main.py --nw_size 8 --coeff 0.2 --beta 0.0
python main.py --nw_size 8 --coeff 0.0 --beta 0.0


수정한내역
1. reward function 수정 필요? - goodput 대신 goodput improvement로 변경 (action 자체가 '변화량'이므로)
    - reward = self.utility_pos_coeff + self.utility_coeff * 20 * (goodput - self.last_goodput) - (1-self.utility_coeff) * action
    - utility_pos_coeff --> [0, 1]
    - 20을 곱해주는건, action이 [-1 1] 사이에서 움직이는데 상대적으로 (goodput - self.last_goodput) 의 변화량이 작아서 변화량의 크기를 맞춰주기 위해
    - utility_pos_coeff = 10 --> reward 음수 안나오는 값으로 때려 박음 -->음수도 상관없으니 0?
2. 현재의 dqn에서 q_value initializing이 zero로 되어 있는지? 한번 max값으로 해보는건 어떨지? --> optimistic initial values
    --> 우선 이건 패스! (Q value는 Weight initial해서 나오는값에서 곱해서나오는거라서, Weight initial은 어차피 0이아니얌 xavier 라는 방법써서 랜덤하게나옴. 따라서 임의의값)
3. [수정완료]max tx_r이 6은 너무 큰 듯? 6이면 3 blocks이라서;; reasonable 실험 setting을 하려면 max 3정도가 적당할듯
5. [수정완료]지금 source의 tx_r은 항상 2로 fix된거 맞음?
4. [수정완료]initial tx_r을 source빼고는 다 0부터 시작하는건 어떨지? --> 수정완료
=====================


2018-2-2 off-line meeting
experiment_setting.txt
exp.sh
참고

2018/01/28 off-line meeting
<민혜>
- explore vs explison 비율 관련 리서치
- 전체 시스템 안정화 시키는 parameter 찾아내기

<주현>
- 변수 모두 세이브하는 기능 구현
- 글로브컴 시그컴 논문에서 딥러닝 내용 확인
- 딥러닝 부분 수준 맞춰서 구현
- 안정적인 학습 가능하도록

2018/01/25

* epsilon의 최적값을 보기 위해 main_v2.py 코드 짜 놓음
* 주현이는 deep RL 구현?

* 추가 연구 포인트 *
- epsilon decay에 대한 리서치를 하는 것이 좋은지?
- goodput과 energy 사이의 trade-off를 보여줄 만한 내용있는지? --> utility_coeff를 사용하는 방법?

=====================
+) A3C --> 노드 생성 및 소멸시 q network를 가져오는 방법
https://www.iros2018.org/ --> 2018/3/1 due
https://icra2018.org/ --> 2018/9/10 due --> 2019년 학회 제출

===========================
2018/01/24

* 민혜생각의 현재의 최적 parameter
one_dim = 7
mu = 4 / 5
init_txr = 2
action_space = [-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1]
환경에서는
    epsilon = 0.4 # explore ratio
    utility_coeff = 2 #0.95  # weight on goodput
    utility_pos_coeff = 1  # to make reward to be positive

* goodput과 energy 사이의 trade-off를 보여줄 만한 내용있는지? --> utility_coeff를 사용하는 방법?

======================
2018/1/21 off-line meeting

Future Schedule 
* 기본 matlab code 완성. 
* 그 후에, 여러 아이디어 discussion을 해서 적용을 해본 후 그 중에 성능 좋은 것을 뽑아냄. 


용어정리
* step: 하나의 action을 취하는 set
* episod: step들의 집합. 우리의 경우는 number of node generation (= network member changes)가 poisson distribution으로 새롭게 나오는 것. 

논문에 추가할 새로운 아이디어들
* explore vs exploit : epsilon값을 constant로 주는 or decay되는 셋팅 --> 이론적인 startegy를 적용 ex) UCB, tomson's sampling 
--> 처음에는 가져다 쓰는것 만으로 ok. best will be 우리 상황에 optimal한 방법을 proof 해주는 것
* multi agent reinforcement learning 
* member change 상황에서 new member나 사라진 멤버들을 어떻게 처리할지에 대한 방법 제안. 
* 아주 나중에 idea: state 의 정의를 adjacent matrix를 가져다 사용하는 건? 
--> 왜 이런생각이 났냐면 deep learning을 쓰려면 high dimension의 input (state definition)이 필요해서!! 

==========================

2018/1/8 conference call
* discussion 내용: 구현 중 Q&A

* 현재 진행 상황:
- Q table 기반 Q-learning 모듈 구현
- Environment 구현, action이 주어지면 그에 따라 reward, next state return하도록
- main 부분 구현하여 전체 구조 잡아놓음 

* state 구현 관련 의문 사항(주현) 및 답변, 피드백(민혜)
- adj_matrix의 diagonal term을 1로 넣어주기
- beta (link failure rate)는 우선 0으로 고정시켜놓고 구현
- player 마다 agent를 의미,
- 각 player 마다 연결된 node 수가 state (state[p] = sum(adj_matrix[p,:])
- action도 각 player마다 tx_r가 있어야함, 처음 initializing 할때만 같게 주어짐.

=================================

2017/11/27 conference call
* discussion 내용: 다른 deep RL 논문의 경우는 state와 action의 수가 매우 많음

* 현재상황: NW environment 부분 python 변환 완료
* next step: RL 부분 코딩 필요 - Q learning algoirhtm - Q table 기반으로 simple example 구현 
--> simple example로 부터 시작해서 점점 더 복잡화 시키는 방향으로 발전

* time line
- 민혜 학회: 12/3 - 9
- 주현 휴가: 12/9 - 16
1. 주현 휴가 전까지 RL 부분 코딩
2. 민혜 주현 휴가동안 코딩 catch up 및 아이디어/발전내용 찾기


==============================
2017/11/11 meeting

* Contents
- Evolutionary Network Formation for Dynamic Wireless Networks with Network Coding 논문 정리 및 Q&A
- paper idea 정리
- discussion 내용
	- state: the number of physically located node
	- action: difference of transmission range
	- reward(R): throughput (can be delayed feedback, should be implemented in simultation code)
	- utility: utility = u + w * R - (1 - w) *a

	- no transition probability (model-free, reinforment learning)
	- experiment setup: change beta(link failure rate), change node location
	- estimate Q(s,a) value based on throughput reward
	- if link failure rate is changed, adapt transmission range by using Q learning based on throughput reward


* To do
민혜:
- 기존 코드 정리 및 업로드 

주현:
- ICML application paper 분석 및 공유
- github 정리
